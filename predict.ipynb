{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wzy\\anaconda3\\envs\\nlu\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaForSequenceClassification\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "model.load_state_dict(torch.load('best.pth'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>premise</th>\n",
       "      <th>hypothesis</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mon Dieu!</td>\n",
       "      <td>This person is speaking English.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>He really shook up my whole mindset, Broker sa...</td>\n",
       "      <td>His mindset never changed, Broker said.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Patients were asked to place themselves on a r...</td>\n",
       "      <td>Most patients rated themselves as a 5 on the s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I managed to pick-pocket someone next to the s...</td>\n",
       "      <td>I stole someone's wallet near the concession s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Forty comments were received and considered pr...</td>\n",
       "      <td>The decisions regarding the issuance of the fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             premise  \\\n",
       "0                                         Mon Dieu!    \n",
       "1  He really shook up my whole mindset, Broker sa...   \n",
       "2  Patients were asked to place themselves on a r...   \n",
       "3  I managed to pick-pocket someone next to the s...   \n",
       "4  Forty comments were received and considered pr...   \n",
       "\n",
       "                                          hypothesis  label  \n",
       "0                   This person is speaking English.      0  \n",
       "1            His mindset never changed, Broker said.      0  \n",
       "2  Most patients rated themselves as a 5 on the s...      1  \n",
       "3  I stole someone's wallet near the concession s...      1  \n",
       "4  The decisions regarding the issuance of the fi...      1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "eval_data = pd.read_csv('Data/dev.csv', encoding='utf-8')\n",
    "eval_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tony  Shoes (so Clinton will have Shoes and Socks).\n",
      "nan\n",
      "tensor([1], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "premise = eval_data['premise'][3126]\n",
    "hypothesis = eval_data['hypothesis'][3126]\n",
    "label = int(eval_data['label'][3126])\n",
    "print(premise)\n",
    "print(hypothesis)\n",
    "encoded_premise = tokenizer(premise,\n",
    "                            \"have shoes\", \n",
    "                            padding='max_length', \n",
    "                            truncation=True, \n",
    "                            max_length=256, \n",
    "                            return_tensors='pt')\n",
    "\n",
    "\n",
    "input_ids = encoded_premise['input_ids'].to(device)\n",
    "attention_mask = encoded_premise['attention_mask'].to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask=attention_mask)\n",
    "preds = torch.argmax(outputs.logits, dim=1)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at index: 3126\n",
      "Premise: Tony  Shoes (so Clinton will have Shoes and Socks).\n",
      "Hypothesis: nan\n",
      "Label: 1\n",
      "Error at index: 3970\n",
      "Premise: Saint-Germain-des-Pr??s\n",
      "Hypothesis: nan\n",
      "Label: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "prediction = []\n",
    "true_labels = []\n",
    "for i in range(len(eval_data)):\n",
    "    premise = eval_data['premise'][i]\n",
    "    hypothesis = eval_data['hypothesis'][i]\n",
    "    label = int(eval_data['label'][i])\n",
    "    try:\n",
    "        encoded_premise = tokenizer(premise,\n",
    "                                    hypothesis, \n",
    "                                    padding='max_length', \n",
    "                                    truncation=True, \n",
    "                                    max_length=256, \n",
    "                                    return_tensors='pt')\n",
    "    except:\n",
    "        print(\"Error at index:\", i)\n",
    "        print(\"Premise:\", premise)\n",
    "        print(\"Hypothesis:\", hypothesis)\n",
    "        print(\"Label:\", label)\n",
    "        if hypothesis == \"nan\":\n",
    "            prediction.append(1)\n",
    "            true_labels.append(label)\n",
    "        continue\n",
    "\n",
    "    input_ids = encoded_premise['input_ids'].to(device)\n",
    "    attention_mask = encoded_premise['attention_mask'].to(device)\n",
    "\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    preds = torch.argmax(outputs.logits, dim=1)\n",
    "    prediction.append(preds.item())\n",
    "    true_labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 0.8807720861172977, F1: 0.8806179535006453, Precision: 0.88073179727605, Recall: 0.8805284630929098\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "val_acc = accuracy_score(true_labels, prediction)\n",
    "f1 = f1_score(true_labels, prediction, average='macro')\n",
    "precision = precision_score(true_labels, prediction, average='macro')\n",
    "recall = recall_score(true_labels, prediction, average='macro')\n",
    "\n",
    "print(f'Acc: {val_acc}, F1: {f1}, Precision: {precision}, Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8807720861172977\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for i in range(len(prediction)):\n",
    "    if prediction[i] == true_labels[i]:\n",
    "        correct += 1\n",
    "\n",
    "print(correct / len(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
